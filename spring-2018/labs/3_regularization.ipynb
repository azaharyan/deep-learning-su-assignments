{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of 3_regularization.ipynb","provenance":[{"file_id":"https://github.com/azaharyan/deep-learning-su-assignments/blob/master/spring-2018/labs/3_regularization.ipynb","timestamp":1587744711894}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"kR-4eNdK6lYS","colab_type":"text"},"source":["Deep Learning\n","=============\n","\n","Assignment 3\n","------------\n","\n","Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n","\n","The goal of this assignment is to explore regularization techniques."]},{"cell_type":"code","metadata":{"id":"JLpLa8Jt7Vu4","colab_type":"code","cellView":"both","outputId":"c24fb1b6-862a-4ce0-cd0f-138867c2c749","executionInfo":{"status":"ok","timestamp":1588787633743,"user_tz":-180,"elapsed":4943,"user":{"displayName":"Alexander Z.","photoUrl":"","userId":"07520320598332507924"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# These are all the modules we'll be using later. Make sure you can import them\n","# before proceeding further.\n","%tensorflow_version 1.x\n","from __future__ import print_function\n","import numpy as np\n","import tensorflow as tf\n","from six.moves import cPickle as pickle"],"execution_count":1,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8s2DEC8H9mDB","colab_type":"code","outputId":"00cc4cc6-8068-4184-df51-9171d959045c","executionInfo":{"status":"ok","timestamp":1588787673697,"user_tz":-180,"elapsed":36909,"user":{"displayName":"Alexander Z.","photoUrl":"","userId":"07520320598332507924"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1HrCK6e17WzV","colab_type":"text"},"source":["First reload the data we generated in `1_notmnist.ipynb`."]},{"cell_type":"code","metadata":{"id":"y3-cj1bpmuxc","colab_type":"code","cellView":"both","executionInfo":{"status":"ok","timestamp":1588787716828,"user_tz":-180,"elapsed":6688,"user":{"displayName":"Alexander Z.","photoUrl":"","userId":"07520320598332507924"}},"outputId":"f7d4c62f-14a7-4e19-b465-04f466151c18","colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["pickle_file = 'drive/My Drive/Colab Notebooks/notMNIST.pickle'\n","\n","with open(pickle_file, 'rb') as f:\n","  save = pickle.load(f)\n","  train_dataset = save['train_dataset']\n","  train_labels = save['train_labels']\n","  valid_dataset = save['valid_dataset']\n","  valid_labels = save['valid_labels']\n","  test_dataset = save['test_dataset']\n","  test_labels = save['test_labels']\n","  del save  # hint to help gc free up memory\n","  print('Training set', train_dataset.shape, train_labels.shape)\n","  print('Validation set', valid_dataset.shape, valid_labels.shape)\n","  print('Test set', test_dataset.shape, test_labels.shape)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Training set (200000, 28, 28) (200000,)\n","Validation set (10000, 28, 28) (10000,)\n","Test set (10000, 28, 28) (10000,)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"L7aHrm6nGDMB","colab_type":"text"},"source":["Reformat into a shape that's more adapted to the models we're going to train:\n","- data as a flat matrix,\n","- labels as float 1-hot encodings."]},{"cell_type":"code","metadata":{"id":"IRSyYiIIGIzS","colab_type":"code","cellView":"both","executionInfo":{"status":"ok","timestamp":1588787720445,"user_tz":-180,"elapsed":1109,"user":{"displayName":"Alexander Z.","photoUrl":"","userId":"07520320598332507924"}},"outputId":"03202cb5-b1f2-4018-af1b-04e0dc360b3d","colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["image_size = 28\n","num_labels = 10\n","\n","def reformat(dataset, labels):\n","  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n","  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n","  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n","  return dataset, labels\n","train_dataset, train_labels = reformat(train_dataset, train_labels)\n","valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n","test_dataset, test_labels = reformat(test_dataset, test_labels)\n","print('Training set', train_dataset.shape, train_labels.shape)\n","print('Validation set', valid_dataset.shape, valid_labels.shape)\n","print('Test set', test_dataset.shape, test_labels.shape)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Training set (200000, 784) (200000, 10)\n","Validation set (10000, 784) (10000, 10)\n","Test set (10000, 784) (10000, 10)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RajPLaL_ZW6w","colab_type":"code","cellView":"both","colab":{}},"source":["def accuracy(predictions, labels):\n","  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n","          / predictions.shape[0])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sgLbUAQ1CW-1","colab_type":"text"},"source":["---\n","Problem 1\n","---------\n","\n","Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n","\n","---"]},{"cell_type":"code","metadata":{"id":"R4kCJ5KrSbnG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":173},"outputId":"1ac28a4a-897b-421b-cb86-779fe58775a8","executionInfo":{"status":"ok","timestamp":1588494678603,"user_tz":-180,"elapsed":6653,"user":{"displayName":"Alexander Z.","photoUrl":"","userId":"07520320598332507924"}}},"source":["batch_size = 128\n","\n","graph = tf.Graph()\n","with graph.as_default():\n","\n","  # Input data. For the training data, we use a placeholder that will be fed\n","  # at run time with a training minibatch.\n","  tf_train_dataset = tf.placeholder(tf.float32,\n","                                    shape=(batch_size, image_size * image_size))\n","  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n","  tf_valid_dataset = tf.constant(valid_dataset)\n","  tf_test_dataset = tf.constant(test_dataset)\n","  \n","  # Variables.\n","  weights = tf.Variable(\n","    tf.truncated_normal([image_size * image_size, num_labels]))\n","  biases = tf.Variable(tf.zeros([num_labels]))\n","  \n","  # Training computation.\n","  logits = tf.matmul(tf_train_dataset, weights) + biases\n","\n","  loss = tf.reduce_mean(\n","    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n","  \n","  regulizer = tf.nn.l2_loss(weights)\n","  loss = tf.reduce_mean(loss + 0.01*regulizer)\n","\n","  # Optimizer.\n","  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n","  \n","  # Predictions for the training, validation, and test data.\n","  train_prediction = tf.nn.softmax(logits)\n","  valid_prediction = tf.nn.softmax(\n","    tf.matmul(tf_valid_dataset, weights) + biases)\n","  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From <ipython-input-6-ded6a13693b8>:23: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","\n","Future major versions of TensorFlow will allow gradients to flow\n","into the labels input on backprop by default.\n","\n","See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gwEVuNNdVWOn","colab_type":"code","outputId":"38caf29e-5a11-4c1f-eee5-0d4a02664c8d","executionInfo":{"status":"ok","timestamp":1587926417318,"user_tz":-180,"elapsed":5286,"user":{"displayName":"Alexander Z.","photoUrl":"","userId":"07520320598332507924"}},"colab":{"base_uri":"https://localhost:8080/","height":408}},"source":["num_steps = 3001\n","\n","with tf.Session(graph=graph) as session:\n","  tf.global_variables_initializer().run()\n","  print(\"Initialized\")\n","  for step in range(num_steps):\n","    # Pick an offset within the training data, which has been randomized.\n","    # Note: we could use better randomization across epochs.\n","    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n","    # Generate a minibatch.\n","    batch_data = train_dataset[offset:(offset + batch_size), :]\n","    batch_labels = train_labels[offset:(offset + batch_size), :]\n","    # Prepare a dictionary telling the session where to feed the minibatch.\n","    # The key of the dictionary is the placeholder node of the graph to be fed,\n","    # and the value is the numpy array to feed to it.\n","    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n","    _, l, predictions = session.run(\n","      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n","    if (step % 500 == 0):\n","      print(\"Minibatch loss at step %d: %f\" % (step, l))\n","      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n","      print(\"Validation accuracy: %.1f%%\" % accuracy(\n","        valid_prediction.eval(), valid_labels))\n","  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Initialized\n","Minibatch loss at step 0: 46.899895\n","Minibatch accuracy: 6.2%\n","Validation accuracy: 9.6%\n","Minibatch loss at step 500: 0.867755\n","Minibatch accuracy: 76.6%\n","Validation accuracy: 81.7%\n","Minibatch loss at step 1000: 0.703149\n","Minibatch accuracy: 82.0%\n","Validation accuracy: 81.5%\n","Minibatch loss at step 1500: 0.941300\n","Minibatch accuracy: 73.4%\n","Validation accuracy: 81.7%\n","Minibatch loss at step 2000: 0.839585\n","Minibatch accuracy: 78.9%\n","Validation accuracy: 80.8%\n","Minibatch loss at step 2500: 0.860183\n","Minibatch accuracy: 78.1%\n","Validation accuracy: 82.1%\n","Minibatch loss at step 3000: 0.806593\n","Minibatch accuracy: 82.0%\n","Validation accuracy: 82.3%\n","Test accuracy: 87.7%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DZg-U0zKWvoH","colab_type":"text"},"source":["# 1-layer NN"]},{"cell_type":"code","metadata":{"id":"ebncgBYxW8G0","colab_type":"code","colab":{}},"source":["def multilayer_perceptron(ds, weights, biases):\n","  layer_1 = tf.matmul(ds, weights['h1']) + biases['h1']\n","  layer_1 = tf.nn.relu(layer_1)\n","  logits = tf.matmul(layer_1, weights['out']) + biases['out']\n","  return logits\n","\n","hidden_layer = 1024\n","batch_size = 128\n","graph = tf.Graph()\n","\n","with graph.as_default():\n","\n","  tf_train_dataset = tf.placeholder(tf.float32,\n","                                    shape=(batch_size, image_size * image_size))\n","  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n","  tf_valid_dataset = tf.constant(valid_dataset)\n","  tf_test_dataset = tf.constant(test_dataset)\n","\n","  weights = {\n","      'h1': tf.Variable(tf.random_normal([image_size*image_size, hidden_layer])),\n","      'out': tf.Variable(tf.random_normal([hidden_layer, num_labels]))\n","  }\n","\n","  biases = {\n","      'h1': tf.Variable(tf.zeros([hidden_layer])),\n","      'out': tf.Variable(tf.zeros([num_labels])) \n","  }\n","\n","  logits = multilayer_perceptron(tf_train_dataset, weights, biases)\n","\n","  loss = tf.reduce_mean(\n","    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n","  \n","  regulizers = tf.nn.l2_loss(weights['h1']) + tf.nn.l2_loss(weights['out'])\n","  loss = tf.reduce_mean(loss + 0.01*regulizers)\n","\n","  # Optimizer.\n","  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n","  \n","  # Predictions for the training, validation, and test data.\n","  train_prediction = tf.nn.softmax(logits)\n","  valid_prediction = tf.nn.softmax(multilayer_perceptron(valid_dataset, weights, biases))\n","  test_prediction = tf.nn.softmax(multilayer_perceptron(test_dataset, weights, biases))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Di_2GWE9W8Du","colab_type":"code","outputId":"2e525c53-fd7c-448e-b33d-6402860e85de","executionInfo":{"status":"ok","timestamp":1587927337995,"user_tz":-180,"elapsed":40604,"user":{"displayName":"Alexander Z.","photoUrl":"","userId":"07520320598332507924"}},"colab":{"base_uri":"https://localhost:8080/","height":408}},"source":["num_steps = 3001\n","\n","with tf.Session(graph=graph) as session:\n","  tf.global_variables_initializer().run()\n","  print(\"Initialized\")\n","  for step in range(num_steps):\n","    # Pick an offset within the training data, which has been randomized.\n","    # Note: we could use better randomization across epochs.\n","    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n","    # Generate a minibatch.\n","    batch_data = train_dataset[offset:(offset + batch_size), :]\n","    batch_labels = train_labels[offset:(offset + batch_size), :]\n","    # Prepare a dictionary telling the session where to feed the minibatch.\n","    # The key of the dictionary is the placeholder node of the graph to be fed,\n","    # and the value is the numpy array to feed to it.\n","    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n","    _, l, predictions = session.run(\n","      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n","    if (step % 500 == 0):\n","      print(\"Minibatch loss at step %d: %f\" % (step, l))\n","      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n","      print(\"Validation accuracy: %.1f%%\" % accuracy(\n","        valid_prediction.eval(), valid_labels))\n","  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Initialized\n","Minibatch loss at step 0: 4500.260254\n","Minibatch accuracy: 9.4%\n","Validation accuracy: 32.6%\n","Minibatch loss at step 500: 27.294937\n","Minibatch accuracy: 82.8%\n","Validation accuracy: 84.5%\n","Minibatch loss at step 1000: 0.854555\n","Minibatch accuracy: 81.2%\n","Validation accuracy: 84.4%\n","Minibatch loss at step 1500: 0.851745\n","Minibatch accuracy: 76.6%\n","Validation accuracy: 84.3%\n","Minibatch loss at step 2000: 0.807461\n","Minibatch accuracy: 78.1%\n","Validation accuracy: 83.9%\n","Minibatch loss at step 2500: 0.782702\n","Minibatch accuracy: 79.7%\n","Validation accuracy: 84.1%\n","Minibatch loss at step 3000: 0.791460\n","Minibatch accuracy: 82.8%\n","Validation accuracy: 84.1%\n","Test accuracy: 89.7%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"na8xX2yHZzNF","colab_type":"text"},"source":["---\n","Problem 2\n","---------\n","Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n","\n","---"]},{"cell_type":"code","metadata":{"id":"bQzHHAMSZjYi","colab_type":"code","outputId":"d081a43f-7452-4498-e031-2ec8dfca4749","executionInfo":{"status":"ok","timestamp":1587927389397,"user_tz":-180,"elapsed":41994,"user":{"displayName":"Alexander Z.","photoUrl":"","userId":"07520320598332507924"}},"colab":{"base_uri":"https://localhost:8080/","height":408}},"source":["num_steps = 3001\n","\n","train_dataset_2 = train_dataset[:500,:]\n","train_labels_2 = train_labels[:500,:]\n","\n","with tf.Session(graph=graph) as session:\n","  tf.global_variables_initializer().run()\n","  print(\"Initialized\")\n","  for step in range(num_steps):\n","    # Pick an offset within the training data, which has been randomized.\n","    # Note: we could use better randomization across epochs.\n","    offset = (step * batch_size) % (train_labels_2.shape[0] - batch_size)\n","    # Generate a minibatch.\n","    batch_data = train_dataset_2[offset:(offset + batch_size), :]\n","    batch_labels = train_labels_2[offset:(offset + batch_size), :]\n","    # Prepare a dictionary telling the session where to feed the minibatch.\n","    # The key of the dictionary is the placeholder node of the graph to be fed,\n","    # and the value is the numpy array to feed to it.\n","    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n","    _, l, predictions = session.run(\n","      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n","    if (step % 500 == 0):\n","      print(\"Minibatch loss at step %d: %f\" % (step, l))\n","      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n","      print(\"Validation accuracy: %.1f%%\" % accuracy(\n","        valid_prediction.eval(), valid_labels))\n","  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Initialized\n","Minibatch loss at step 0: 4509.837891\n","Minibatch accuracy: 10.2%\n","Validation accuracy: 38.5%\n","Minibatch loss at step 500: 27.216579\n","Minibatch accuracy: 100.0%\n","Validation accuracy: 77.5%\n","Minibatch loss at step 1000: 0.485874\n","Minibatch accuracy: 100.0%\n","Validation accuracy: 78.5%\n","Minibatch loss at step 1500: 0.275697\n","Minibatch accuracy: 100.0%\n","Validation accuracy: 78.8%\n","Minibatch loss at step 2000: 0.265946\n","Minibatch accuracy: 100.0%\n","Validation accuracy: 78.7%\n","Minibatch loss at step 2500: 0.255457\n","Minibatch accuracy: 100.0%\n","Validation accuracy: 78.7%\n","Minibatch loss at step 3000: 0.250052\n","Minibatch accuracy: 100.0%\n","Validation accuracy: 78.7%\n","Test accuracy: 85.3%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5MUKqaJcacfS","colab_type":"text"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"0Sr7M24caetk","colab_type":"text"},"source":["Due to the limited number of examples in the training set, the model is able to learn it very well (100% minibatch accuracy), but the validation accuracy is really low. That is a sign of overfitting!"]},{"cell_type":"markdown","metadata":{"id":"ww3SCBUdlkRc","colab_type":"text"},"source":["---\n","Problem 3\n","---------\n","Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n","\n","What happens to our extreme overfitting case?\n","\n","---"]},{"cell_type":"code","metadata":{"id":"IWqXv4Vth8Oh","colab_type":"code","colab":{}},"source":["def multilayer_perceptron(ds, weights, biases):\n","  layer_1 = tf.matmul(ds, weights['h1']) + biases['h1']\n","  layer_1 = tf.nn.relu(layer_1)\n","  logits = tf.matmul(layer_1, weights['out']) + biases['out']\n","  return logits\n","\n","hidden_layer = 1024\n","batch_size = 128\n","graph = tf.Graph()\n","\n","with graph.as_default():\n","\n","  tf_train_dataset = tf.placeholder(tf.float32,\n","                                    shape=(batch_size, image_size * image_size))\n","  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n","  tf_valid_dataset = tf.constant(valid_dataset)\n","  tf_test_dataset = tf.constant(test_dataset)\n","\n","  weights = {\n","      'h1': tf.Variable(tf.random_normal([image_size*image_size, hidden_layer])),\n","      'out': tf.Variable(tf.random_normal([hidden_layer, num_labels]))\n","  }\n","\n","  biases = {\n","      'h1': tf.Variable(tf.zeros([hidden_layer])),\n","      'out': tf.Variable(tf.zeros([num_labels])) \n","  }\n","\n","  layer_1 = tf.matmul(tf_train_dataset, weights['h1']) + biases['h1']\n","  relu_layer_1 = tf.nn.relu(layer_1)\n","  dropout_layer_1 = tf.nn.dropout(relu_layer_1, rate=0.5)\n","  logits = tf.matmul(layer_1, weights['out']) + biases['out']\n","\n","  loss = tf.reduce_mean(\n","    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n","  \n","  regulizers = tf.nn.l2_loss(weights['h1']) + tf.nn.l2_loss(weights['out'])\n","  loss = tf.reduce_mean(loss + 0.01*regulizers)\n","\n","  # Optimizer.\n","  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n","  \n","  # Predictions for the training, validation, and test data.\n","  train_prediction = tf.nn.softmax(logits)\n","  valid_prediction = tf.nn.softmax(multilayer_perceptron(valid_dataset, weights, biases))\n","  test_prediction = tf.nn.softmax(multilayer_perceptron(test_dataset, weights, biases))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mTXG9ODHi4Dw","colab_type":"code","outputId":"593d7f1b-56ec-4b08-9987-df84ef3ced74","executionInfo":{"status":"ok","timestamp":1587754666171,"user_tz":-180,"elapsed":38305,"user":{"displayName":"Alexander Z.","photoUrl":"","userId":"07520320598332507924"}},"colab":{"base_uri":"https://localhost:8080/","height":408}},"source":["num_steps = 3001\n","\n","with tf.Session(graph=graph) as session:\n","  tf.global_variables_initializer().run()\n","  print(\"Initialized\")\n","  for step in range(num_steps):\n","    # Pick an offset within the training data, which has been randomized.\n","    # Note: we could use better randomization across epochs.\n","    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n","    # Generate a minibatch.\n","    batch_data = train_dataset[offset:(offset + batch_size), :]\n","    batch_labels = train_labels[offset:(offset + batch_size), :]\n","    # Prepare a dictionary telling the session where to feed the minibatch.\n","    # The key of the dictionary is the placeholder node of the graph to be fed,\n","    # and the value is the numpy array to feed to it.\n","    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n","    _, l, predictions = session.run(\n","      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n","    if (step % 500 == 0):\n","      print(\"Minibatch loss at step %d: %f\" % (step, l))\n","      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n","      print(\"Validation accuracy: %.1f%%\" % accuracy(\n","        valid_prediction.eval(), valid_labels))\n","  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Initialized\n","Minibatch loss at step 0: 4629.563477\n","Minibatch accuracy: 10.2%\n","Validation accuracy: 41.4%\n","Minibatch loss at step 500: 26.688364\n","Minibatch accuracy: 75.8%\n","Validation accuracy: 77.3%\n","Minibatch loss at step 1000: 0.921974\n","Minibatch accuracy: 79.7%\n","Validation accuracy: 80.7%\n","Minibatch loss at step 1500: 0.993358\n","Minibatch accuracy: 75.8%\n","Validation accuracy: 79.5%\n","Minibatch loss at step 2000: 0.894462\n","Minibatch accuracy: 79.7%\n","Validation accuracy: 80.3%\n","Minibatch loss at step 2500: 0.936820\n","Minibatch accuracy: 78.1%\n","Validation accuracy: 81.2%\n","Minibatch loss at step 3000: 0.854817\n","Minibatch accuracy: 81.2%\n","Validation accuracy: 80.7%\n","Test accuracy: 86.7%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yxIQEBN4jfZh","colab_type":"text"},"source":["###Extreme overfitting"]},{"cell_type":"code","metadata":{"id":"DB8mGCDSjrJX","colab_type":"code","outputId":"53ac2be4-6cc7-43aa-bf02-ce9f7f8e7288","executionInfo":{"status":"ok","timestamp":1587754822140,"user_tz":-180,"elapsed":38951,"user":{"displayName":"Alexander Z.","photoUrl":"","userId":"07520320598332507924"}},"colab":{"base_uri":"https://localhost:8080/","height":408}},"source":["num_steps = 3001\n","\n","train_dataset_2 = train_dataset[:500,:]\n","train_labels_2 = train_labels[:500,:]\n","\n","with tf.Session(graph=graph) as session:\n","  tf.global_variables_initializer().run()\n","  print(\"Initialized\")\n","  for step in range(num_steps):\n","    # Pick an offset within the training data, which has been randomized.\n","    # Note: we could use better randomization across epochs.\n","    offset = (step * batch_size) % (train_labels_2.shape[0] - batch_size)\n","    # Generate a minibatch.\n","    batch_data = train_dataset_2[offset:(offset + batch_size), :]\n","    batch_labels = train_labels_2[offset:(offset + batch_size), :]\n","    # Prepare a dictionary telling the session where to feed the minibatch.\n","    # The key of the dictionary is the placeholder node of the graph to be fed,\n","    # and the value is the numpy array to feed to it.\n","    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n","    _, l, predictions = session.run(\n","      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n","    if (step % 500 == 0):\n","      print(\"Minibatch loss at step %d: %f\" % (step, l))\n","      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n","      print(\"Validation accuracy: %.1f%%\" % accuracy(\n","        valid_prediction.eval(), valid_labels))\n","  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Initialized\n","Minibatch loss at step 0: 4554.845703\n","Minibatch accuracy: 16.4%\n","Validation accuracy: 31.9%\n","Minibatch loss at step 500: 27.337843\n","Minibatch accuracy: 100.0%\n","Validation accuracy: 73.2%\n","Minibatch loss at step 1000: 0.444055\n","Minibatch accuracy: 100.0%\n","Validation accuracy: 75.8%\n","Minibatch loss at step 1500: 0.256356\n","Minibatch accuracy: 100.0%\n","Validation accuracy: 76.0%\n","Minibatch loss at step 2000: 0.266266\n","Minibatch accuracy: 100.0%\n","Validation accuracy: 76.2%\n","Minibatch loss at step 2500: 0.256901\n","Minibatch accuracy: 100.0%\n","Validation accuracy: 76.4%\n","Minibatch loss at step 3000: 0.256145\n","Minibatch accuracy: 100.0%\n","Validation accuracy: 76.4%\n","Test accuracy: 83.2%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-b1hTz3VWZjw","colab_type":"text"},"source":["---\n","Problem 4\n","---------\n","\n","Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n","\n","One avenue you can explore is to add multiple layers.\n","\n","Another one is to use learning rate decay:\n","\n","    global_step = tf.Variable(0)  # count the number of steps taken.\n","    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n","    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n"," \n"," ---\n"]},{"cell_type":"code","metadata":{"id":"lLA2fQ1Pnn6F","colab_type":"code","colab":{}},"source":["import math as math\n","\n","def two_layer_train(ds, weights, biases):\n","  layer_1 = tf.matmul(ds, weights['h1']) + biases['h1']\n","  relu_layer_1 = tf.nn.relu(layer_1)\n","\n","  layer_2 = tf.matmul(relu_layer_1, weights['h2']) + biases['h2']\n","  relu_layer_2 = tf.nn.relu(layer_2)\n","\n","  layer_3 = tf.matmul(relu_layer_2, weights['h3']) + biases['h3']\n","  relu_layer_3 = tf.nn.relu(layer_3)\n","\n","  layer_4 = tf.matmul(relu_layer_3, weights['h4']) + biases['h4']\n","  relu_layer_4 = tf.nn.relu(layer_4)\n","  logits = tf.matmul(relu_layer_4, weights['out']) + biases['out']\n","  return logits\n","\n","\n","hidden_layer_1 = 4096\n","hidden_layer_2 = 2048\n","hidden_layer_3 = 512\n","hidden_layer_4 = 256\n","\n","batch_size = 256\n","\n","graph = tf.Graph()\n","with graph.as_default():\n","\n","  tf_train_dataset = tf.placeholder(tf.float32, shape=[batch_size,image_size*image_size])\n","  tf_train_labels = tf.placeholder(tf.float32, shape=[batch_size, num_labels])\n","\n","  tf_valid_dataset = tf.constant(valid_dataset)\n","  tf_test_dataset = tf.constant(test_dataset)\n","\n","  weights = {\n","      'h1': tf.Variable(tf.truncated_normal([image_size*image_size, hidden_layer_1], stddev=math.sqrt(2.0/(image_size*image_size)))),\n","      'h2': tf.Variable(tf.truncated_normal([hidden_layer_1, hidden_layer_2], stddev=math.sqrt(2.0/(image_size*image_size)))),\n","      'h3': tf.Variable(tf.truncated_normal([hidden_layer_2, hidden_layer_3], stddev=math.sqrt(2.0/(image_size*image_size)))),\n","      'h4': tf.Variable(tf.truncated_normal([hidden_layer_3, hidden_layer_4], stddev=math.sqrt(2.0/(image_size*image_size)))),\n","      'out': tf.Variable(tf.truncated_normal([hidden_layer_4, num_labels], stddev=math.sqrt(2.0/(image_size*image_size))))\n","  }\n","\n","  biases = {\n","      'h1': tf.Variable(tf.zeros([hidden_layer_1])),\n","      'h2': tf.Variable(tf.zeros([hidden_layer_2])),\n","      'h3': tf.Variable(tf.zeros([hidden_layer_3])),\n","      'h4': tf.Variable(tf.zeros([hidden_layer_4])),\n","      'out': tf.Variable(tf.zeros([num_labels]))\n","  }\n","\n","  layer_1 = tf.matmul(tf_train_dataset, weights['h1']) + biases['h1']\n","  relu_layer_1 = tf.nn.relu(layer_1)\n","\n","  layer_2 = tf.matmul(relu_layer_1, weights['h2']) + biases['h2']\n","  relu_layer_2 = tf.nn.relu(layer_2)\n","\n","  layer_3 = tf.matmul(relu_layer_2, weights['h3']) + biases['h3']\n","  relu_layer_3 = tf.nn.relu(layer_3)\n","\n","  layer_4 = tf.matmul(relu_layer_3, weights['h4']) + biases['h4']\n","  relu_layer_4 = tf.nn.relu(layer_4)\n","\n","  logits = tf.matmul(relu_layer_4, weights['out']) + biases['out']\n","\n","  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n","\n","  weights_reg = [tf.nn.l2_loss(weigth) for weigth in weights.values()]\n","  biases_reg = [tf.nn.l2_loss(bias) for bias in biases.values()]\n","  regulizers = sum(weights_reg) + sum(biases_reg)\n","  loss = tf.reduce_mean(loss + 0.001*regulizers)\n","\n","  optimizer = tf.train.GradientDescentOptimizer(0.01).minimize(loss)\n","  \n","  \n","  # Predictions for the training, validation, and test data.\n","  train_prediction = tf.nn.softmax(logits)\n","  valid_prediction = tf.nn.softmax(two_layer_train(valid_dataset, weights, biases))\n","  test_prediction = tf.nn.softmax(two_layer_train(test_dataset, weights, biases))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nvkityGSurFL","colab_type":"code","outputId":"a6d074f6-9050-4780-8575-737d257e32d9","executionInfo":{"status":"ok","timestamp":1588801993105,"user_tz":-180,"elapsed":3319597,"user":{"displayName":"Alexander Z.","photoUrl":"","userId":"07520320598332507924"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["num_steps = 9001\n","\n","with tf.Session(graph=graph) as session:\n","  tf.global_variables_initializer().run()\n","  print(\"Initialized\")\n","  for step in range(num_steps):\n","    # Pick an offset within the training data, which has been randomized.\n","    # Note: we could use better randomization across epochs.\n","    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n","    # Generate a minibatch.\n","    batch_data = train_dataset[offset:(offset + batch_size), :]\n","    batch_labels = train_labels[offset:(offset + batch_size), :]\n","    # Prepare a dictionary telling the session where to feed the minibatch.\n","    # The key of the dictionary is the placeholder node of the graph to be fed,\n","    # and the value is the numpy array to feed to it.\n","    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n","    _, l, predictions = session.run(\n","      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n","    if (step % 500 == 0):\n","      print(\"Minibatch loss at step %d: %f\" % (step, l))\n","      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n","      print(\"Validation accuracy: %.1f%%\" % accuracy(\n","        valid_prediction.eval(), valid_labels))\n","  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"],"execution_count":25,"outputs":[{"output_type":"stream","text":["Initialized\n","Minibatch loss at step 0: 15.114830\n","Minibatch accuracy: 7.4%\n","Validation accuracy: 8.6%\n","Minibatch loss at step 500: 12.967811\n","Minibatch accuracy: 87.1%\n","Validation accuracy: 83.7%\n","Minibatch loss at step 1000: 12.950684\n","Minibatch accuracy: 84.0%\n","Validation accuracy: 84.7%\n","Minibatch loss at step 1500: 12.725400\n","Minibatch accuracy: 85.2%\n","Validation accuracy: 85.8%\n","Minibatch loss at step 2000: 12.534329\n","Minibatch accuracy: 87.9%\n","Validation accuracy: 86.5%\n","Minibatch loss at step 2500: 12.404662\n","Minibatch accuracy: 91.0%\n","Validation accuracy: 86.9%\n","Minibatch loss at step 3000: 12.394123\n","Minibatch accuracy: 86.3%\n","Validation accuracy: 87.2%\n","Minibatch loss at step 3500: 12.183247\n","Minibatch accuracy: 87.1%\n","Validation accuracy: 87.5%\n","Minibatch loss at step 4000: 12.223462\n","Minibatch accuracy: 83.6%\n","Validation accuracy: 87.8%\n","Minibatch loss at step 4500: 11.897497\n","Minibatch accuracy: 89.8%\n","Validation accuracy: 88.3%\n","Minibatch loss at step 5000: 11.770333\n","Minibatch accuracy: 89.8%\n","Validation accuracy: 88.3%\n","Minibatch loss at step 5500: 11.649530\n","Minibatch accuracy: 91.8%\n","Validation accuracy: 88.7%\n","Minibatch loss at step 6000: 11.615065\n","Minibatch accuracy: 86.3%\n","Validation accuracy: 88.8%\n","Minibatch loss at step 6500: 11.403515\n","Minibatch accuracy: 91.4%\n","Validation accuracy: 88.9%\n","Minibatch loss at step 7000: 11.189836\n","Minibatch accuracy: 94.5%\n","Validation accuracy: 88.9%\n","Minibatch loss at step 7500: 11.265678\n","Minibatch accuracy: 87.9%\n","Validation accuracy: 88.9%\n","Minibatch loss at step 8000: 11.090885\n","Minibatch accuracy: 90.2%\n","Validation accuracy: 89.3%\n","Minibatch loss at step 8500: 11.005756\n","Minibatch accuracy: 89.5%\n","Validation accuracy: 89.3%\n","Minibatch loss at step 9000: 10.898818\n","Minibatch accuracy: 90.6%\n","Validation accuracy: 89.3%\n","Test accuracy: 94.5%\n"],"name":"stdout"}]}]}