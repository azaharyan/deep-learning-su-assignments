{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"cnn.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"4embtkV0pNxM"},"source":["Deep Learning with Tensorflow\n","=============\n","\n","Assignment II\n","------------\n","\n","During one of the lectures in [Lab 1](https://deep-learning-su.github.io/labs/lab-1/) we trained fully connected network to classify [notMNIST](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html) characters. \n","\n","The goal of this assignment is make the neural network convolutional.\n","\n","For this exercise, you would need the `notMNIST.pickle` created in `Lab 1`. You can obtain it by rerunning the given paragraphs without having to solve the problems (although it is highly recommended to do it if you haven't already)."]},{"cell_type":"code","metadata":{"cellView":"both","colab_type":"code","id":"tm2CQN_Cpwj0","outputId":"3e92e910-9ce1-4fb9-a847-958f3facd89f","executionInfo":{"status":"ok","timestamp":1590602825821,"user_tz":-180,"elapsed":6072,"user":{"displayName":"Alexander Z.","photoUrl":"","userId":"07520320598332507924"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# These are all the modules we'll be using later. Make sure you can import them\n","# before proceeding further.\n","%tensorflow_version 1.x\n","from __future__ import print_function\n","import numpy as np\n","import tensorflow as tf\n","from six.moves import cPickle as pickle\n","from six.moves import range"],"execution_count":1,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Qt0qQyMNwARA","colab_type":"code","outputId":"ca5d8f67-a3e4-4baf-92c7-54b858caa25f","executionInfo":{"status":"ok","timestamp":1590602877665,"user_tz":-180,"elapsed":37779,"user":{"displayName":"Alexander Z.","photoUrl":"","userId":"07520320598332507924"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"cellView":"both","colab_type":"code","id":"y3-cj1bpmuxc","outputId":"da183e15-91c3-40db-aae2-30cadb6cd921","executionInfo":{"status":"ok","timestamp":1590602902883,"user_tz":-180,"elapsed":14800,"user":{"displayName":"Alexander Z.","photoUrl":"","userId":"07520320598332507924"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["pickle_file = 'drive/My Drive/Colab Notebooks/notMNIST.pickle'\n","\n","with open(pickle_file, 'rb') as f:\n","  save = pickle.load(f)\n","  train_dataset = save['train_dataset']\n","  train_labels = save['train_labels']\n","  valid_dataset = save['valid_dataset']\n","  valid_labels = save['valid_labels']\n","  test_dataset = save['test_dataset']\n","  test_labels = save['test_labels']\n","  del save  # hint to help gc free up memory\n","  print('Training set', train_dataset.shape, train_labels.shape)\n","  print('Validation set', valid_dataset.shape, valid_labels.shape)\n","  print('Test set', test_dataset.shape, test_labels.shape)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Training set (200000, 28, 28) (200000,)\n","Validation set (10000, 28, 28) (10000,)\n","Test set (10000, 28, 28) (10000,)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"L7aHrm6nGDMB"},"source":["Reformat into a TensorFlow-friendly shape:\n","- convolutions need the image data formatted as a cube (width by height by #channels)\n","- labels as float 1-hot encodings."]},{"cell_type":"code","metadata":{"cellView":"both","colab_type":"code","id":"IRSyYiIIGIzS","outputId":"ceb9f1fc-c53c-4588-87b8-13b335cc5777","executionInfo":{"status":"ok","timestamp":1590602967685,"user_tz":-180,"elapsed":2715,"user":{"displayName":"Alexander Z.","photoUrl":"","userId":"07520320598332507924"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["image_size = 28\n","num_labels = 10\n","num_channels = 1 # grayscale\n","\n","import numpy as np\n","\n","def reformat(dataset, labels):\n","  dataset = dataset.reshape(\n","    (-1, image_size, image_size, num_channels)).astype(np.float32)\n","  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n","  return dataset, labels\n","train_dataset, train_labels = reformat(train_dataset, train_labels)\n","valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n","test_dataset, test_labels = reformat(test_dataset, test_labels)\n","print('Training set', train_dataset.shape, train_labels.shape)\n","print('Validation set', valid_dataset.shape, valid_labels.shape)\n","print('Test set', test_dataset.shape, test_labels.shape)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Training set (200000, 28, 28, 1) (200000, 10)\n","Validation set (10000, 28, 28, 1) (10000, 10)\n","Test set (10000, 28, 28, 1) (10000, 10)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"cellView":"both","colab_type":"code","id":"AgQDIREv02p1","colab":{}},"source":["def accuracy(predictions, labels):\n","  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n","          / predictions.shape[0])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"5rhgjmROXu2O"},"source":["## Problem 1\n","Let's build a small network with two convolutional layers, followed by one fully connected layer. Convolutional networks are more expensive computationally, so we'll limit its depth and number of fully connected nodes.\n","\n","Edit the snippet bellow by changing the `model` function.\n","\n","### 1.1 - Define the model\n","Implement the `model` function bellow. Take a look at the following TF functions:\n","- **tf.nn.conv2d(X,W1, strides = [1,s,s,1], padding = 'SAME'):** given an input $X$ and a group of filters $W1$, this function convolves $W1$'s filters on X. The third input ([1,f,f,1]) represents the strides for each dimension of the input (m, n_H_prev, n_W_prev, n_C_prev). You can read the full documentation [here](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d)\n","- **tf.nn.relu(Z1):** computes the elementwise ReLU of Z1 (which can be any shape). You can read the full documentation [here.](https://www.tensorflow.org/api_docs/python/tf/nn/relu)\n","\n","### 1.2 - Compute loss\n","\n","Implement the `compute_loss` function below. You might find these two functions helpful: \n","\n","- **tf.nn.softmax_cross_entropy_with_logits(logits = Z3, labels = Y):** computes the softmax entropy loss. This function both computes the softmax activation function as well as the resulting loss. You can check the full documentation  [here.](https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits)\n","- **tf.reduce_mean:** computes the mean of elements across dimensions of a tensor. Use this to sum the losses over all the examples to get the overall cost. You can check the full documentation [here.](https://www.tensorflow.org/api_docs/python/tf/reduce_mean)\n"]},{"cell_type":"code","metadata":{"id":"v66i0vzGuqJI","colab_type":"code","colab":{}},"source":["def calculate_output_size(input_size, filter_size, padding, stride):\n","  if padding == 'same':\n","    padding = 1.00\n","  elif padding == 'valid':\n","    padding = 0.00\n","  else:\n","    return None\n","\n","  output_1 = float(((input_size - filter_size + 2*padding)/stride) + 1.00)\n","  output_2 = float(((output_1 - filter_size + 2*padding)/stride) + 1.00)\n","\n","  return (int(np.ceil(output_1)), int(np.ceil(output_2)))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"both","colab_type":"code","id":"IZYv70SvvOan","outputId":"badfc21f-426e-44d1-9584-8aee96cdf49f","executionInfo":{"status":"ok","timestamp":1590324338711,"user_tz":-180,"elapsed":910,"user":{"displayName":"Alexander Z.","photoUrl":"","userId":"07520320598332507924"}},"colab":{"base_uri":"https://localhost:8080/","height":136}},"source":["import math as math\n","\n","batch_size = 16\n","patch_size = 5\n","depth = 16 # Number of filters?\n","num_hidden = 64 # Size of the fully connected layer?\n","\n","graph = tf.Graph()\n","\n","with graph.as_default():\n","\n","  # Input data.\n","  tf_train_dataset = tf.placeholder(\n","    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n","  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n","  tf_valid_dataset = tf.constant(valid_dataset)\n","  tf_test_dataset = tf.constant(test_dataset)\n","\n","  #Variables\n","  final_conv_output_size = calculate_output_size(image_size, patch_size, padding='same', stride=2)[1]\n","  print(final_conv_output_size)\n","\n","  weights = {\n","      'layer1': tf.Variable(tf.truncated_normal([patch_size,patch_size,num_channels, depth], stddev=math.sqrt(2.0/(image_size*image_size)))),\n","      'layer2': tf.Variable(tf.truncated_normal([patch_size, patch_size,depth, depth], stddev=math.sqrt(2.0/(image_size*image_size)))),\n","      'layer3': tf.Variable(tf.truncated_normal([final_conv_output_size*final_conv_output_size*depth, num_hidden],stddev=math.sqrt(2.0/(image_size*image_size)))),\n","      'layer4': tf.Variable(tf.truncated_normal([num_hidden, num_labels], stddev=math.sqrt(2.0/(image_size*image_size))))\n","  }\n","\n","  biases = {\n","      'layer1': tf.Variable(tf.zeros([depth])),\n","      'layer2': tf.Variable(tf.zeros([depth])),\n","      'layer3': tf.Variable(tf.zeros([num_hidden])),\n","      'layer4': tf.Variable(tf.zeros([num_labels]))\n","  }\n","  \n","  # Model.\n","  def model(data):\n","    # define a simple network with \n","    # * 2 convolutional layers with 5x5 filters each using stride 2 and zero padding\n","    # * one fully connected layer\n","    # return the logits (last layer)   \n","    conv1 = tf.nn.conv2d(data, weights['layer1'], strides=[1,2,2,1], padding='SAME')\n","    relu1 = tf.nn.relu(conv1 + biases['layer1'])\n","    conv2 = tf.nn.conv2d(relu1, weights['layer2'], strides=[1,2,2,1], padding='SAME')\n","    relu2 = tf.nn.relu(conv2 + biases['layer2'])\n","\n","    shape = relu2.get_shape().as_list()\n","    print(shape)\n","    reshape = tf.reshape(relu2, [shape[0], shape[1]* shape[2]*shape[3]])\n","    print(reshape.get_shape())\n","    fully_connected = tf.nn.relu(tf.matmul(reshape, weights['layer3']) + biases['layer3'])\n","\n","    return tf.matmul(fully_connected, weights['layer4']) + biases['layer4']\n","\n","  def compute_loss(labels, logits):\n","    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n","  \n","  # Training computation.\n","  logits = model(tf_train_dataset)\n","  loss = compute_loss(tf_train_labels, logits)\n","    \n","  # Optimizer.\n","  optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n","  \n","  # Predictions for the training, validation, and test data.\n","  train_prediction = tf.nn.softmax(logits)\n","  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n","  test_prediction = tf.nn.softmax(model(tf_test_dataset))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["7\n","[16, 7, 7, 16]\n","(16, 784)\n","[10000, 7, 7, 16]\n","(10000, 784)\n","[10000, 7, 7, 16]\n","(10000, 784)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZkzpbHET-m8S","colab_type":"text"},"source":["### 1.3 - Measure the accuracy and tune your model\n","\n","Run the snippet bellow to measure the accuracy of your model. Try to achieve a test accuracy of around 80%. Iterate on the filters size."]},{"cell_type":"code","metadata":{"cellView":"both","colab_type":"code","id":"noKFb2UovVFR","outputId":"ca129aea-b0a0-4308-c33d-ddaa3a566f2a","executionInfo":{"status":"ok","timestamp":1590324369283,"user_tz":-180,"elapsed":26957,"user":{"displayName":"Alexander Z.","photoUrl":"","userId":"07520320598332507924"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["num_steps = 1001\n","\n","with tf.Session(graph=graph) as session:\n","  tf.global_variables_initializer().run()\n","  print('Initialized')\n","  for step in range(num_steps):\n","    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n","    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n","    batch_labels = train_labels[offset:(offset + batch_size), :]\n","    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n","    _, l, predictions = session.run(\n","      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n","    if (step % 50 == 0):\n","      print('Minibatch loss at step %d: %f' % (step, l))\n","      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n","      print('Validation accuracy: %.1f%%' % accuracy(\n","        valid_prediction.eval(), valid_labels))\n","  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Initialized\n","Minibatch loss at step 0: 2.303803\n","Minibatch accuracy: 12.5%\n","Validation accuracy: 11.6%\n","Minibatch loss at step 50: 2.186541\n","Minibatch accuracy: 6.2%\n","Validation accuracy: 36.5%\n","Minibatch loss at step 100: 0.996398\n","Minibatch accuracy: 56.2%\n","Validation accuracy: 60.5%\n","Minibatch loss at step 150: 1.635663\n","Minibatch accuracy: 43.8%\n","Validation accuracy: 73.5%\n","Minibatch loss at step 200: 0.610735\n","Minibatch accuracy: 75.0%\n","Validation accuracy: 78.5%\n","Minibatch loss at step 250: 1.065750\n","Minibatch accuracy: 68.8%\n","Validation accuracy: 79.7%\n","Minibatch loss at step 300: 0.660847\n","Minibatch accuracy: 81.2%\n","Validation accuracy: 76.0%\n","Minibatch loss at step 350: 0.786066\n","Minibatch accuracy: 81.2%\n","Validation accuracy: 80.5%\n","Minibatch loss at step 400: 0.651812\n","Minibatch accuracy: 81.2%\n","Validation accuracy: 80.4%\n","Minibatch loss at step 450: 0.575303\n","Minibatch accuracy: 81.2%\n","Validation accuracy: 81.9%\n","Minibatch loss at step 500: 1.836814\n","Minibatch accuracy: 50.0%\n","Validation accuracy: 76.2%\n","Minibatch loss at step 550: 1.430478\n","Minibatch accuracy: 56.2%\n","Validation accuracy: 81.6%\n","Minibatch loss at step 600: 1.061697\n","Minibatch accuracy: 62.5%\n","Validation accuracy: 82.1%\n","Minibatch loss at step 650: 0.183099\n","Minibatch accuracy: 100.0%\n","Validation accuracy: 82.8%\n","Minibatch loss at step 700: 1.125067\n","Minibatch accuracy: 56.2%\n","Validation accuracy: 83.5%\n","Minibatch loss at step 750: 0.376705\n","Minibatch accuracy: 93.8%\n","Validation accuracy: 83.6%\n","Minibatch loss at step 800: 0.682603\n","Minibatch accuracy: 75.0%\n","Validation accuracy: 83.1%\n","Minibatch loss at step 850: 0.218881\n","Minibatch accuracy: 87.5%\n","Validation accuracy: 84.0%\n","Minibatch loss at step 900: 0.610936\n","Minibatch accuracy: 87.5%\n","Validation accuracy: 84.3%\n","Minibatch loss at step 950: 0.425851\n","Minibatch accuracy: 87.5%\n","Validation accuracy: 84.8%\n","Minibatch loss at step 1000: 0.984691\n","Minibatch accuracy: 68.8%\n","Validation accuracy: 83.2%\n","Test accuracy: 89.4%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"KedKkn4EutIK"},"source":["---\n","Problem 2\n","---------\n","\n","The convolutional model above uses convolutions with stride 2 to reduce the dimensionality. Replace the strides by a max pooling operation (`nn.max_pool()`) of stride 2 and kernel size 2.\n","\n","---"]},{"cell_type":"code","metadata":{"id":"CvnwYq57uvgZ","colab_type":"code","outputId":"76952ea0-2ba3-485e-879a-b612fb3912f1","executionInfo":{"status":"ok","timestamp":1590434794730,"user_tz":-180,"elapsed":1138,"user":{"displayName":"Alexander Z.","photoUrl":"","userId":"07520320598332507924"}},"colab":{"base_uri":"https://localhost:8080/","height":136}},"source":["import math as math\n","\n","batch_size = 16\n","patch_size = 5\n","depth = 16 # Number of filters?\n","num_hidden = 64 # Size of the fully connected layer?\n","\n","graph = tf.Graph()\n","\n","with graph.as_default():\n","\n","  # Input data.\n","  tf_train_dataset = tf.placeholder(\n","    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n","  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n","  tf_valid_dataset = tf.constant(valid_dataset)\n","  tf_test_dataset = tf.constant(test_dataset)\n","\n","  #Variables\n","  final_conv_output_size = calculate_output_size(image_size, patch_size, padding='same', stride=2)[1]\n","  print(final_conv_output_size)\n","\n","  weights = {\n","      'layer1': tf.Variable(tf.truncated_normal([patch_size,patch_size,num_channels, depth], stddev=math.sqrt(2.0/(image_size*image_size)))),\n","      'layer2': tf.Variable(tf.truncated_normal([patch_size, patch_size,depth, depth], stddev=math.sqrt(2.0/(image_size*image_size)))),\n","      'layer3': tf.Variable(tf.truncated_normal([final_conv_output_size*final_conv_output_size*depth, num_hidden],stddev=math.sqrt(2.0/(image_size*image_size)))),\n","      'layer4': tf.Variable(tf.truncated_normal([num_hidden, num_labels], stddev=math.sqrt(2.0/(image_size*image_size))))\n","  }\n","\n","  biases = {\n","      'layer1': tf.Variable(tf.zeros([depth])),\n","      'layer2': tf.Variable(tf.zeros([depth])),\n","      'layer3': tf.Variable(tf.zeros([num_hidden])),\n","      'layer4': tf.Variable(tf.zeros([num_labels]))\n","  }\n","  \n","  # Model.\n","  def model(data):\n","    # define a simple network with \n","    # * 2 convolutional layers with 5x5 filters each using stride 2 and zero padding\n","    # * one fully connected layer\n","    # return the logits (last layer)   \n","    conv1 = tf.nn.conv2d(data, weights['layer1'], strides=[1,1,1,1], padding='SAME')\n","    relu1 = tf.nn.relu(conv1 + biases['layer1'])\n","    pool = tf.nn.max_pool(relu1, ksize=2, strides=2, padding='SAME')\n","    conv2 = tf.nn.conv2d(pool, weights['layer2'], strides=[1,1,1,1], padding='SAME')\n","    relu2 = tf.nn.relu(conv2 + biases['layer2'])\n","    pool = tf.nn.max_pool(relu2, ksize=2, strides=2, padding='SAME')\n","\n","    shape = pool.get_shape().as_list()\n","    print(shape)\n","    reshape = tf.reshape(pool, [shape[0], shape[1]* shape[2]*shape[3]])\n","    print(reshape.get_shape())\n","    fully_connected = tf.nn.relu(tf.matmul(reshape, weights['layer3']) + biases['layer3'])\n","\n","    return tf.matmul(fully_connected, weights['layer4']) + biases['layer4']\n","\n","  def compute_loss(labels, logits):\n","    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n","  \n","  # Training computation.\n","  logits = model(tf_train_dataset)\n","  loss = compute_loss(tf_train_labels, logits)\n","    \n","  # Optimizer.\n","  optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n","  \n","  # Predictions for the training, validation, and test data.\n","  train_prediction = tf.nn.softmax(logits)\n","  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n","  test_prediction = tf.nn.softmax(model(tf_test_dataset))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["7\n","[16, 7, 7, 16]\n","(16, 784)\n","[10000, 7, 7, 16]\n","(10000, 784)\n","[10000, 7, 7, 16]\n","(10000, 784)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xt5RJNYVvZxM","colab_type":"code","outputId":"36182b31-2ae8-405c-bf6f-2ead2445e421","executionInfo":{"status":"ok","timestamp":1590435359442,"user_tz":-180,"elapsed":73476,"user":{"displayName":"Alexander Z.","photoUrl":"","userId":"07520320598332507924"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["num_steps = 1001\n","\n","with tf.Session(graph=graph) as session:\n","  tf.global_variables_initializer().run()\n","  print('Initialized')\n","  for step in range(num_steps):\n","    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n","    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n","    batch_labels = train_labels[offset:(offset + batch_size), :]\n","    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n","    _, l, predictions = session.run(\n","      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n","    if (step % 50 == 0):\n","      print('Minibatch loss at step %d: %f' % (step, l))\n","      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n","      print('Validation accuracy: %.1f%%' % accuracy(\n","        valid_prediction.eval(), valid_labels))\n","  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Initialized\n","Minibatch loss at step 0: 2.298245\n","Minibatch accuracy: 6.2%\n","Validation accuracy: 11.7%\n","Minibatch loss at step 50: 2.186566\n","Minibatch accuracy: 25.0%\n","Validation accuracy: 21.1%\n","Minibatch loss at step 100: 1.037658\n","Minibatch accuracy: 56.2%\n","Validation accuracy: 62.4%\n","Minibatch loss at step 150: 1.985950\n","Minibatch accuracy: 43.8%\n","Validation accuracy: 68.8%\n","Minibatch loss at step 200: 0.493282\n","Minibatch accuracy: 87.5%\n","Validation accuracy: 79.2%\n","Minibatch loss at step 250: 0.906434\n","Minibatch accuracy: 68.8%\n","Validation accuracy: 80.7%\n","Minibatch loss at step 300: 0.666051\n","Minibatch accuracy: 81.2%\n","Validation accuracy: 81.6%\n","Minibatch loss at step 350: 0.691479\n","Minibatch accuracy: 81.2%\n","Validation accuracy: 82.0%\n","Minibatch loss at step 400: 0.672642\n","Minibatch accuracy: 87.5%\n","Validation accuracy: 82.7%\n","Minibatch loss at step 450: 0.434093\n","Minibatch accuracy: 87.5%\n","Validation accuracy: 83.9%\n","Minibatch loss at step 500: 1.871957\n","Minibatch accuracy: 56.2%\n","Validation accuracy: 71.1%\n","Minibatch loss at step 550: 1.101277\n","Minibatch accuracy: 62.5%\n","Validation accuracy: 82.8%\n","Minibatch loss at step 600: 0.910118\n","Minibatch accuracy: 75.0%\n","Validation accuracy: 79.0%\n","Minibatch loss at step 650: 0.141769\n","Minibatch accuracy: 93.8%\n","Validation accuracy: 84.2%\n","Minibatch loss at step 700: 1.049571\n","Minibatch accuracy: 56.2%\n","Validation accuracy: 85.1%\n","Minibatch loss at step 750: 0.472149\n","Minibatch accuracy: 87.5%\n","Validation accuracy: 84.7%\n","Minibatch loss at step 800: 0.588686\n","Minibatch accuracy: 81.2%\n","Validation accuracy: 85.0%\n","Minibatch loss at step 850: 0.277797\n","Minibatch accuracy: 93.8%\n","Validation accuracy: 85.0%\n","Minibatch loss at step 900: 0.405921\n","Minibatch accuracy: 87.5%\n","Validation accuracy: 85.0%\n","Minibatch loss at step 950: 0.374560\n","Minibatch accuracy: 87.5%\n","Validation accuracy: 86.3%\n","Minibatch loss at step 1000: 1.142582\n","Minibatch accuracy: 68.8%\n","Validation accuracy: 84.3%\n","Test accuracy: 89.8%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"klf21gpbAgb-"},"source":["---\n","Problem 3\n","---------\n","\n","Try to get the best performance you can using a convolutional net. Look for example at the classic [LeNet5](http://yann.lecun.com/exdb/lenet/) architecture, adding Dropout, and/or adding learning rate decay.\n","\n","---"]},{"cell_type":"code","metadata":{"id":"OXsurJo9QBQt","colab_type":"code","colab":{}},"source":["image_size = 28\n","\n","def output_size_pool(input_size, conv_filter_size, pool_filter_size, padding, conv_stride, pool_stride):\n","    if padding == 'same':\n","        padding = -1.00\n","    elif padding == 'valid':\n","        padding = 0.00\n","    else:\n","        return None\n","    # After convolution 1\n","    output_1 = (((input_size - conv_filter_size - 2*padding) / conv_stride) + 1.00)\n","    # After pool 1\n","    output_2 = (((output_1 - pool_filter_size - 2*padding) / pool_stride) + 1.00)    \n","    # After convolution 2\n","    output_3 = (((output_2 - conv_filter_size - 2*padding) / conv_stride) + 1.00)\n","    # After pool 2\n","    output_4 = (((output_3 - pool_filter_size - 2*padding) / pool_stride) + 1.00)  \n","    return int(output_4)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RrmxnRGY-m8X","colab_type":"code","outputId":"8b4fe861-858a-4d3f-ac45-a2ccdfa98c6e","executionInfo":{"status":"ok","timestamp":1590433979905,"user_tz":-180,"elapsed":938,"user":{"displayName":"Alexander Z.","photoUrl":"","userId":"07520320598332507924"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["import math as math\n","\n","batch_size = 32\n","patch_size = 5\n","depth = 32\n","depth2 = 32 # Number of filters?\n","num_hidden1 = 120\n","num_hidden2 = 84 # Size of the fully connected layer?\n","\n","graph = tf.Graph()\n","\n","with graph.as_default():\n","\n","  # Input data.\n","  tf_train_dataset = tf.placeholder(\n","    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n","  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n","  tf_valid_dataset = tf.constant(valid_dataset)\n","  tf_test_dataset = tf.constant(test_dataset)\n","\n","  #Variables\n","  final_conv_output_size = output_size_pool(input_size=image_size, conv_filter_size=5, pool_filter_size=2, padding='valid', conv_stride=1, pool_stride=2)\n","  print(final_conv_output_size)\n","\n","  weights = {\n","      'layer1': tf.Variable(tf.truncated_normal([patch_size,patch_size,num_channels, depth], stddev=math.sqrt(2.0/(image_size*image_size)))),\n","      'layer2': tf.Variable(tf.truncated_normal([patch_size, patch_size,depth, depth2], stddev=math.sqrt(2.0/(image_size*image_size)))),\n","      'layer3': tf.Variable(tf.truncated_normal([final_conv_output_size*final_conv_output_size*depth2, num_hidden1],stddev=math.sqrt(2.0/(image_size*image_size)))),\n","      'layer4': tf.Variable(tf.truncated_normal([num_hidden1, num_hidden2], stddev=math.sqrt(2.0/(image_size*image_size)))),\n","      'layer5': tf.Variable(tf.truncated_normal([num_hidden2, num_labels], stddev=math.sqrt(2.0/(image_size*image_size))))\n","  }\n","\n","  biases = {\n","      'layer1': tf.Variable(tf.zeros([depth])),\n","      'layer2': tf.Variable(tf.zeros([depth2])),\n","      'layer3': tf.Variable(tf.zeros([num_hidden1])),\n","      'layer4': tf.Variable(tf.zeros([num_hidden2])),\n","      'layer5': tf.Variable(tf.zeros([num_labels]))\n","  }\n","  \n","  # Model.\n","  def model(data):\n","    # define a simple network with \n","    # * 2 convolutional layers with 5x5 filters each using stride 2 and zero padding\n","    # * one fully connected layer\n","    # return the logits (last layer)   \n","    conv1 = tf.nn.conv2d(data, weights['layer1'], strides=[1,1,1,1], padding='VALID')\n","    tanh1 = tf.nn.relu(conv1 + biases['layer1'])\n","    pool = tf.nn.max_pool(tanh1, ksize=2, strides=2, padding='VALID')\n","    conv2 = tf.nn.conv2d(pool, weights['layer2'], strides=[1,1,1,1], padding='VALID')\n","    tanh2 = tf.nn.relu(conv2 + biases['layer2'])\n","    pool = tf.nn.max_pool(tanh2, ksize=2, strides=2, padding='VALID')\n","\n","    shape = pool.get_shape().as_list()\n","    print(shape)\n","    reshape = tf.reshape(pool, [shape[0], shape[1]* shape[2]*shape[3]])\n","    fully_connected = tf.nn.relu(tf.matmul(reshape, weights['layer3']) + biases['layer3'])\n","    keep_prob = 0.6\n","    fully_connected = tf.nn.dropout(fully_connected, keep_prob)\n","    fully_connected2 = tf.nn.relu(tf.matmul(fully_connected, weights['layer4']) + biases['layer4'])\n","    fully_connected2 = tf.nn.dropout(fully_connected2, keep_prob)\n","    return tf.matmul(fully_connected2, weights['layer5']) + biases['layer5']\n","\n","  def compute_loss(labels, logits):\n","    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n","  \n","  # Training computation.\n","  logits = model(tf_train_dataset)\n","  loss = compute_loss(tf_train_labels, logits)\n","    \n","  # Optimizer.\n","  global_step = tf.Variable(0)  # count the number of steps taken.\n","  start_learning_rate = 0.05\n","  learning_rate = tf.train.exponential_decay(start_learning_rate, global_step, 100000, 0.96, staircase=True)\n","\n","  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n","  \n","  # Predictions for the training, validation, and test data.\n","  train_prediction = tf.nn.softmax(logits)\n","  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n","  test_prediction = tf.nn.softmax(model(tf_test_dataset))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["4\n","[32, 4, 4, 32]\n","[10000, 4, 4, 32]\n","[10000, 4, 4, 32]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nxYnpGfxF75Z","colab_type":"code","outputId":"6fad89b6-cb65-4d3a-f70d-d67eabb53c16","executionInfo":{"status":"ok","timestamp":1590434755496,"user_tz":-180,"elapsed":772963,"user":{"displayName":"Alexander Z.","photoUrl":"","userId":"07520320598332507924"}},"colab":{"base_uri":"https://localhost:8080/","height":408}},"source":["num_steps = 30001\n","\n","with tf.Session(graph=graph) as session:\n","  tf.global_variables_initializer().run()\n","  print('Initialized')\n","  for step in range(num_steps):\n","    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n","    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n","    batch_labels = train_labels[offset:(offset + batch_size), :]\n","    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n","    _, l, predictions = session.run(\n","      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n","    if (step % 5000 == 0):\n","      print('Minibatch loss at step %d: %f' % (step, l))\n","      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n","      print('Validation accuracy: %.1f%%' % accuracy(\n","        valid_prediction.eval(), valid_labels))\n","  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Initialized\n","Minibatch loss at step 0: 2.303037\n","Minibatch accuracy: 3.1%\n","Validation accuracy: 10.6%\n","Minibatch loss at step 5000: 0.557899\n","Minibatch accuracy: 84.4%\n","Validation accuracy: 86.9%\n","Minibatch loss at step 10000: 0.440327\n","Minibatch accuracy: 84.4%\n","Validation accuracy: 88.0%\n","Minibatch loss at step 15000: 0.421590\n","Minibatch accuracy: 81.2%\n","Validation accuracy: 88.8%\n","Minibatch loss at step 20000: 0.062344\n","Minibatch accuracy: 100.0%\n","Validation accuracy: 89.2%\n","Minibatch loss at step 25000: 0.127938\n","Minibatch accuracy: 93.8%\n","Validation accuracy: 89.7%\n","Minibatch loss at step 30000: 0.421327\n","Minibatch accuracy: 87.5%\n","Validation accuracy: 90.0%\n","Test accuracy: 94.8%\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZDzIhp7bVOjh","colab_type":"code","outputId":"c76aa925-4cb5-4166-f131-a1828b8334e0","executionInfo":{"status":"ok","timestamp":1590603025455,"user_tz":-180,"elapsed":2520,"user":{"displayName":"Alexander Z.","photoUrl":"","userId":"07520320598332507924"}},"colab":{"base_uri":"https://localhost:8080/","height":343}},"source":["import math as math\n","\n","batch_size = 128\n","patch_size = 5\n","depth = 16 # Number of filters?\n","num_hidden = 120\n","num_hidden2 = 84 # Size of the fully connected layer?\n","\n","graph = tf.Graph()\n","\n","with graph.as_default():\n","\n","  # Input data.\n","  tf_train_dataset = tf.placeholder(\n","    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n","  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n","  tf_valid_dataset = tf.constant(valid_dataset)\n","  tf_test_dataset = tf.constant(test_dataset)\n","\n","  #Variables\n","  final_conv_output_size = calculate_output_size(image_size, patch_size, padding='same', stride=2)[1]\n","  print(final_conv_output_size)\n","\n","  weights = {\n","      'layer1': tf.Variable(tf.truncated_normal([patch_size,patch_size,num_channels, depth], stddev=math.sqrt(2.0/(image_size*image_size)))),\n","      'layer2': tf.Variable(tf.truncated_normal([patch_size, patch_size,depth, depth], stddev=math.sqrt(2.0/(image_size*image_size)))),\n","      'layer3': tf.Variable(tf.truncated_normal([final_conv_output_size*final_conv_output_size*depth, num_hidden],stddev=math.sqrt(2.0/(image_size*image_size)))),\n","      'layer4': tf.Variable(tf.truncated_normal([num_hidden, num_hidden2], stddev=math.sqrt(2.0/(image_size*image_size)))),\n","      'layer5': tf.Variable(tf.truncated_normal([num_hidden2, num_labels], stddev=math.sqrt(2.0/(image_size*image_size))))\n","  }\n","\n","  biases = {\n","      'layer1': tf.Variable(tf.zeros([depth])),\n","      'layer2': tf.Variable(tf.zeros([depth])),\n","      'layer3': tf.Variable(tf.zeros([num_hidden])),\n","      'layer4': tf.Variable(tf.zeros([num_hidden2])),\n","      'layer5': tf.Variable(tf.zeros([num_labels]))\n","  }\n","  \n","  # Model.\n","  def model(data):\n","    # define a simple network with \n","    # * 2 convolutional layers with 5x5 filters each using stride 2 and zero padding\n","    # * one fully connected layer\n","    # return the logits (last layer)   \n","    conv1 = tf.nn.conv2d(data, weights['layer1'], strides=[1,1,1,1], padding='SAME')\n","    relu1 = tf.nn.relu(conv1 + biases['layer1'])\n","    pool = tf.nn.max_pool(relu1, ksize=2, strides=2, padding='SAME')\n","    conv2 = tf.nn.conv2d(pool, weights['layer2'], strides=[1,1,1,1], padding='SAME')\n","    relu2 = tf.nn.relu(conv2 + biases['layer2'])\n","    pool = tf.nn.max_pool(relu2, ksize=2, strides=2, padding='SAME')\n","\n","    shape = pool.get_shape().as_list()\n","    print(shape)\n","    reshape = tf.reshape(pool, [shape[0], shape[1]* shape[2]*shape[3]])\n","    print(reshape.get_shape())\n","    fully_connected = tf.nn.relu(tf.matmul(reshape, weights['layer3']) + biases['layer3'])\n","    fully_connected = tf.nn.dropout(fully_connected, 0.8)\n","    fully_connected = tf.nn.relu(tf.matmul(fully_connected, weights['layer4']) + biases['layer4'])\n","    fully_connected = tf.nn.dropout(fully_connected, 0.8)\n","\n","    return tf.matmul(fully_connected, weights['layer5']) + biases['layer5']\n","\n","  def compute_loss(labels, logits):\n","    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n","  \n","  # Training computation.\n","  logits = model(tf_train_dataset)\n","  loss = compute_loss(tf_train_labels, logits)\n","    \n","  # Optimizer.\n","  optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n","  \n","  # Predictions for the training, validation, and test data.\n","  train_prediction = tf.nn.softmax(logits)\n","  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n","  test_prediction = tf.nn.softmax(model(tf_test_dataset))"],"execution_count":8,"outputs":[{"output_type":"stream","text":["7\n","[128, 7, 7, 16]\n","(128, 784)\n","WARNING:tensorflow:From <ipython-input-8-5772c33a6dc9>:58: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","WARNING:tensorflow:From <ipython-input-8-5772c33a6dc9>:65: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","\n","Future major versions of TensorFlow will allow gradients to flow\n","into the labels input on backprop by default.\n","\n","See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n","\n","[10000, 7, 7, 16]\n","(10000, 784)\n","[10000, 7, 7, 16]\n","(10000, 784)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ptlCyAFnV_qk","colab_type":"code","outputId":"844419d3-1c8f-4d69-e46c-e72cc39288e7","colab":{"base_uri":"https://localhost:8080/","height":408},"executionInfo":{"status":"ok","timestamp":1590611553562,"user_tz":-180,"elapsed":3541647,"user":{"displayName":"Alexander Z.","photoUrl":"","userId":"07520320598332507924"}}},"source":["num_steps = 30001\n","\n","with tf.Session(graph=graph) as session:\n","  tf.global_variables_initializer().run()\n","  print('Initialized')\n","  for step in range(num_steps):\n","    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n","    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n","    batch_labels = train_labels[offset:(offset + batch_size), :]\n","    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n","    _, l, predictions = session.run(\n","      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n","    if (step % 5000 == 0):\n","      print('Minibatch loss at step %d: %f' % (step, l))\n","      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n","      print('Validation accuracy: %.1f%%' % accuracy(\n","        valid_prediction.eval(), valid_labels))\n","  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Initialized\n","Minibatch loss at step 0: 2.303313\n","Minibatch accuracy: 13.3%\n","Validation accuracy: 10.8%\n","Minibatch loss at step 5000: 0.408128\n","Minibatch accuracy: 85.9%\n","Validation accuracy: 89.8%\n","Minibatch loss at step 10000: 0.209394\n","Minibatch accuracy: 93.8%\n","Validation accuracy: 90.8%\n","Minibatch loss at step 15000: 0.195737\n","Minibatch accuracy: 93.0%\n","Validation accuracy: 91.0%\n","Minibatch loss at step 20000: 0.238617\n","Minibatch accuracy: 91.4%\n","Validation accuracy: 91.3%\n","Minibatch loss at step 25000: 0.176566\n","Minibatch accuracy: 93.8%\n","Validation accuracy: 91.3%\n","Minibatch loss at step 30000: 0.218945\n","Minibatch accuracy: 93.8%\n","Validation accuracy: 91.6%\n","Test accuracy: 96.2%\n"],"name":"stdout"}]}]}