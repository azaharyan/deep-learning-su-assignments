{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of 2_fullyconnected.ipynb","provenance":[{"file_id":"https://github.com/azaharyan/deep-learning-su-assignments/blob/master/spring-2018/labs/2_fullyconnected.ipynb","timestamp":1587741953569}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"kR-4eNdK6lYS","colab_type":"text"},"source":["Deep Learning\n","=============\n","\n","Assignment 2\n","------------\n","\n","Previously in `1_notmnist.ipynb`, we created a pickle with formatted datasets for training, development and testing on the [notMNIST dataset](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html).\n","\n","The goal of this assignment is to progressively train deeper and more accurate models using TensorFlow."]},{"cell_type":"code","metadata":{"id":"JLpLa8Jt7Vu4","colab_type":"code","cellView":"both","outputId":"3fb9c521-a7f1-4630-9fb6-71d892c484af","executionInfo":{"status":"ok","timestamp":1587742957996,"user_tz":-180,"elapsed":5479,"user":{"displayName":"Alexander Z.","photoUrl":"","userId":"07520320598332507924"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# These are all the modules we'll be using later. Make sure you can import them\n","# before proceeding further.\n","%tensorflow_version 1.x\n","from __future__ import print_function\n","import numpy as np\n","import tensorflow as tf\n","from six.moves import cPickle as pickle\n","from six.moves import range"],"execution_count":0,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3vM97szi0gJ9","colab_type":"code","outputId":"428cb7bb-ec5b-4a5d-f4a4-61ea855d19a4","executionInfo":{"status":"ok","timestamp":1587742958003,"user_tz":-180,"elapsed":1253,"user":{"displayName":"Alexander Z.","photoUrl":"","userId":"07520320598332507924"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1HrCK6e17WzV","colab_type":"text"},"source":["First reload the data we generated in `1_notmnist.ipynb`."]},{"cell_type":"code","metadata":{"id":"y3-cj1bpmuxc","colab_type":"code","cellView":"both","executionInfo":{"status":"ok","timestamp":1587742967272,"user_tz":-180,"elapsed":2566,"user":{"displayName":"Alexander Z.","photoUrl":"","userId":"07520320598332507924"}},"outputId":"fee88719-003e-483a-9a8f-7b10a8315b39","colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["pickle_file = 'drive/My Drive/Colab Notebooks/notMNIST.pickle'\n","\n","with open(pickle_file, 'rb') as f:\n","  save = pickle.load(f)\n","  train_dataset = save['train_dataset']\n","  train_labels = save['train_labels']\n","  valid_dataset = save['valid_dataset']\n","  valid_labels = save['valid_labels']\n","  test_dataset = save['test_dataset']\n","  test_labels = save['test_labels']\n","  del save  # hint to help gc free up memory\n","  print('Training set', train_dataset.shape, train_labels.shape)\n","  print('Validation set', valid_dataset.shape, valid_labels.shape)\n","  print('Test set', test_dataset.shape, test_labels.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Training set (200000, 28, 28) (200000,)\n","Validation set (10000, 28, 28) (10000,)\n","Test set (10000, 28, 28) (10000,)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"L7aHrm6nGDMB","colab_type":"text"},"source":["Reformat into a shape that's more adapted to the models we're going to train:\n","- data as a flat matrix,\n","- labels as float 1-hot encodings."]},{"cell_type":"code","metadata":{"id":"IRSyYiIIGIzS","colab_type":"code","cellView":"both","executionInfo":{"status":"ok","timestamp":1587742977385,"user_tz":-180,"elapsed":965,"user":{"displayName":"Alexander Z.","photoUrl":"","userId":"07520320598332507924"}},"outputId":"aeb6c9cf-3095-407f-e66c-f5851fd6ce96","colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["image_size = 28\n","num_labels = 10\n","\n","def reformat(dataset, labels):\n","  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n","  # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n","  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n","  return dataset, labels\n","train_dataset, train_labels = reformat(train_dataset, train_labels)\n","valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n","test_dataset, test_labels = reformat(test_dataset, test_labels)\n","print('Training set', train_dataset.shape, train_labels.shape)\n","print('Validation set', valid_dataset.shape, valid_labels.shape)\n","print('Test set', test_dataset.shape, test_labels.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Training set (200000, 784) (200000, 10)\n","Validation set (10000, 784) (10000, 10)\n","Test set (10000, 784) (10000, 10)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nCLVqyQ5vPPH","colab_type":"text"},"source":["We're first going to train a multinomial logistic regression using simple gradient descent.\n","\n","TensorFlow works like this:\n","* First you describe the computation that you want to see performed: what the inputs, the variables, and the operations look like. These get created as nodes over a computation graph. This description is all contained within the block below:\n","\n","      with graph.as_default():\n","          ...\n","\n","* Then you can run the operations on this graph as many times as you want by calling `session.run()`, providing it outputs to fetch from the graph that get returned. This runtime operation is all contained in the block below:\n","\n","      with tf.Session(graph=graph) as session:\n","          ...\n","\n","Let's load all the data into TensorFlow and build the computation graph corresponding to our training:"]},{"cell_type":"code","metadata":{"id":"Nfv39qvtvOl_","colab_type":"code","cellView":"both","outputId":"968eae81-8591-493a-ee8a-6e7057a54267","executionInfo":{"status":"ok","timestamp":1587742982878,"user_tz":-180,"elapsed":989,"user":{"displayName":"Alexander Z.","photoUrl":"","userId":"07520320598332507924"}},"colab":{"base_uri":"https://localhost:8080/","height":173}},"source":["# With gradient descent training, even this much data is prohibitive.\n","# Subset the training data for faster turnaround.\n","train_subset = 10000\n","\n","graph = tf.Graph()\n","with graph.as_default():\n","\n","  # Input data.\n","  # Load the training, validation and test data into constants that are\n","  # attached to the graph.\n","  tf_train_dataset = tf.constant(train_dataset[:train_subset, :])\n","  tf_train_labels = tf.constant(train_labels[:train_subset])\n","  tf_valid_dataset = tf.constant(valid_dataset)\n","  tf_test_dataset = tf.constant(test_dataset)\n","  \n","  # Variables.\n","  # These are the parameters that we are going to be training. The weight\n","  # matrix will be initialized using random values following a (truncated)\n","  # normal distribution. The biases get initialized to zero.\n","  weights = tf.Variable(\n","    tf.truncated_normal([image_size * image_size, num_labels]))\n","  biases = tf.Variable(tf.zeros([num_labels]))\n","  \n","  # Training computation.\n","  # We multiply the inputs with the weight matrix, and add biases. We compute\n","  # the softmax and cross-entropy (it's one operation in TensorFlow, because\n","  # it's very common, and it can be optimized). We take the average of this\n","  # cross-entropy across all training examples: that's our loss.\n","  logits = tf.matmul(tf_train_dataset, weights) + biases\n","  loss = tf.reduce_mean(\n","    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n","  \n","  # Optimizer.\n","  # We are going to find the minimum of this loss using gradient descent.\n","  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n","  \n","  # Predictions for the training, validation, and test data.\n","  # These are not part of training, but merely here so that we can report\n","  # accuracy figures as we train.\n","  train_prediction = tf.nn.softmax(logits)\n","  valid_prediction = tf.nn.softmax(\n","    tf.matmul(tf_valid_dataset, weights) + biases)\n","  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From <ipython-input-5-58acd45c85e4>:29: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","\n","Future major versions of TensorFlow will allow gradients to flow\n","into the labels input on backprop by default.\n","\n","See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KQcL4uqISHjP","colab_type":"text"},"source":["Let's run this computation and iterate:"]},{"cell_type":"code","metadata":{"id":"z2cjdenH869W","colab_type":"code","cellView":"both","executionInfo":{"status":"ok","timestamp":1587743016068,"user_tz":-180,"elapsed":25822,"user":{"displayName":"Alexander Z.","photoUrl":"","userId":"07520320598332507924"}},"outputId":"d6b775f1-d675-4da4-b6dd-39ecf1e23399","colab":{"base_uri":"https://localhost:8080/","height":510}},"source":["num_steps = 801\n","\n","def accuracy(predictions, labels):\n","  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n","          / predictions.shape[0])\n","\n","with tf.Session(graph=graph) as session:\n","  # This is a one-time operation which ensures the parameters get initialized as\n","  # we described in the graph: random weights for the matrix, zeros for the\n","  # biases. \n","  tf.global_variables_initializer().run()\n","  print('Initialized')\n","  for step in range(num_steps):\n","    # Run the computations. We tell .run() that we want to run the optimizer,\n","    # and get the loss value and the training predictions returned as numpy\n","    # arrays.\n","    _, l, predictions = session.run([optimizer, loss, train_prediction])\n","    if (step % 100 == 0):\n","      print('Loss at step %d: %f' % (step, l))\n","      print('Training accuracy: %.1f%%' % accuracy(\n","        predictions, train_labels[:train_subset, :]))\n","      # Calling .eval() on valid_prediction is basically like calling run(), but\n","      # just to get that one numpy array. Note that it recomputes all its graph\n","      # dependencies.\n","      print('Validation accuracy: %.1f%%' % accuracy(\n","        valid_prediction.eval(), valid_labels))\n","  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Initialized\n","Loss at step 0: 15.959743\n","Training accuracy: 11.6%\n","Validation accuracy: 14.3%\n","Loss at step 100: 2.306348\n","Training accuracy: 71.6%\n","Validation accuracy: 70.9%\n","Loss at step 200: 1.846805\n","Training accuracy: 74.8%\n","Validation accuracy: 73.8%\n","Loss at step 300: 1.600999\n","Training accuracy: 76.3%\n","Validation accuracy: 74.6%\n","Loss at step 400: 1.438102\n","Training accuracy: 77.1%\n","Validation accuracy: 74.9%\n","Loss at step 500: 1.319836\n","Training accuracy: 77.8%\n","Validation accuracy: 75.1%\n","Loss at step 600: 1.228520\n","Training accuracy: 78.5%\n","Validation accuracy: 75.4%\n","Loss at step 700: 1.154566\n","Training accuracy: 79.0%\n","Validation accuracy: 75.6%\n","Loss at step 800: 1.092678\n","Training accuracy: 79.4%\n","Validation accuracy: 75.9%\n","Test accuracy: 82.9%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"x68f-hxRGm3H","colab_type":"text"},"source":["Let's now switch to stochastic gradient descent training instead, which is much faster.\n","\n","The graph will be similar, except that instead of holding all the training data into a constant node, we create a `Placeholder` node which will be fed actual data at every call of `session.run()`."]},{"cell_type":"code","metadata":{"id":"qhPMzWYRGrzM","colab_type":"code","cellView":"both","colab":{}},"source":["batch_size = 128\n","\n","graph = tf.Graph()\n","with graph.as_default():\n","\n","  # Input data. For the training data, we use a placeholder that will be fed\n","  # at run time with a training minibatch.\n","  tf_train_dataset = tf.placeholder(tf.float32,\n","                                    shape=(batch_size, image_size * image_size))\n","  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n","  tf_valid_dataset = tf.constant(valid_dataset)\n","  tf_test_dataset = tf.constant(test_dataset)\n","  \n","  # Variables.\n","  weights = tf.Variable(\n","    tf.truncated_normal([image_size * image_size, num_labels]))\n","  biases = tf.Variable(tf.zeros([num_labels]))\n","  \n","  # Training computation.\n","  logits = tf.matmul(tf_train_dataset, weights) + biases\n","  loss = tf.reduce_mean(\n","    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n","  \n","  # Optimizer.\n","  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n","  \n","  # Predictions for the training, validation, and test data.\n","  train_prediction = tf.nn.softmax(logits)\n","  valid_prediction = tf.nn.softmax(\n","    tf.matmul(tf_valid_dataset, weights) + biases)\n","  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XmVZESmtG4JH","colab_type":"text"},"source":["Let's run it:"]},{"cell_type":"code","metadata":{"id":"FoF91pknG_YW","colab_type":"code","cellView":"both","executionInfo":{"status":"ok","timestamp":1587743193646,"user_tz":-180,"elapsed":5358,"user":{"displayName":"Alexander Z.","photoUrl":"","userId":"07520320598332507924"}},"outputId":"934546c5-eb7e-4ef8-c3f5-88b621c99387","colab":{"base_uri":"https://localhost:8080/","height":408}},"source":["num_steps = 3001\n","\n","with tf.Session(graph=graph) as session:\n","  tf.global_variables_initializer().run()\n","  print(\"Initialized\")\n","  for step in range(num_steps):\n","    # Pick an offset within the training data, which has been randomized.\n","    # Note: we could use better randomization across epochs.\n","    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n","    # Generate a minibatch.\n","    batch_data = train_dataset[offset:(offset + batch_size), :]\n","    batch_labels = train_labels[offset:(offset + batch_size), :]\n","    # Prepare a dictionary telling the session where to feed the minibatch.\n","    # The key of the dictionary is the placeholder node of the graph to be fed,\n","    # and the value is the numpy array to feed to it.\n","    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n","    _, l, predictions = session.run(\n","      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n","    if (step % 500 == 0):\n","      print(\"Minibatch loss at step %d: %f\" % (step, l))\n","      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n","      print(\"Validation accuracy: %.1f%%\" % accuracy(\n","        valid_prediction.eval(), valid_labels))\n","  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Initialized\n","Minibatch loss at step 0: 16.111721\n","Minibatch accuracy: 10.2%\n","Validation accuracy: 13.4%\n","Minibatch loss at step 500: 1.559971\n","Minibatch accuracy: 71.1%\n","Validation accuracy: 76.5%\n","Minibatch loss at step 1000: 1.085004\n","Minibatch accuracy: 80.5%\n","Validation accuracy: 77.0%\n","Minibatch loss at step 1500: 1.484856\n","Minibatch accuracy: 70.3%\n","Validation accuracy: 77.8%\n","Minibatch loss at step 2000: 1.138362\n","Minibatch accuracy: 76.6%\n","Validation accuracy: 77.9%\n","Minibatch loss at step 2500: 1.088993\n","Minibatch accuracy: 75.0%\n","Validation accuracy: 78.6%\n","Minibatch loss at step 3000: 1.116238\n","Minibatch accuracy: 76.6%\n","Validation accuracy: 79.2%\n","Test accuracy: 85.0%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7omWxtvLLxik","colab_type":"text"},"source":["---\n","Problem\n","-------\n","\n","Turn the logistic regression example with SGD into a 1-hidden layer neural network with rectified linear units [nn.relu()](https://www.tensorflow.org/versions/r0.7/api_docs/python/nn.html#relu) and 1024 hidden nodes. This model should improve your validation / test accuracy.\n","\n","---"]},{"cell_type":"code","metadata":{"id":"Ho5de9-8w6LU","colab_type":"code","colab":{}},"source":["def multilayer_perceptron(ds, weights, biases):\n","  layer_1 = tf.matmul(ds, weights['h1']) + biases['h1']\n","  layer_1 = tf.nn.relu(layer_1)\n","  logits = tf.matmul(layer_1, weights['out']) + biases['out']\n","  return logits\n","\n","hidden_layer = 1024\n","batch_size = 128\n","graph = tf.Graph()\n","\n","with graph.as_default():\n","\n","  tf_train_dataset = tf.placeholder(tf.float32,\n","                                    shape=(batch_size, image_size * image_size))\n","  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n","  tf_valid_dataset = tf.constant(valid_dataset)\n","  tf_test_dataset = tf.constant(test_dataset)\n","\n","  weights = {\n","      'h1': tf.Variable(tf.random_normal([image_size*image_size, hidden_layer])),\n","      'out': tf.Variable(tf.random_normal([hidden_layer, num_labels]))\n","  }\n","\n","  biases = {\n","      'h1': tf.Variable(tf.random_normal([hidden_layer])),\n","      'out': tf.Variable(tf.random_normal([num_labels])) \n","  }\n","\n","  logits = multilayer_perceptron(tf_train_dataset, weights, biases)\n","\n","  loss = tf.reduce_mean(\n","    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n","  \n","  # Optimizer.\n","  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n","  \n","  # Predictions for the training, validation, and test data.\n","  train_prediction = tf.nn.softmax(logits)\n","  valid_prediction = tf.nn.softmax(multilayer_perceptron(valid_dataset, weights, biases))\n","  test_prediction = tf.nn.softmax(multilayer_perceptron(test_dataset, weights, biases))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bd8hgW030Qvc","colab_type":"code","outputId":"5483cb66-1f6f-4215-8372-ffb65135813d","executionInfo":{"status":"ok","timestamp":1587743430334,"user_tz":-180,"elapsed":70007,"user":{"displayName":"Alexander Z.","photoUrl":"","userId":"07520320598332507924"}},"colab":{"base_uri":"https://localhost:8080/","height":408}},"source":["num_steps = 3001\n","\n","with tf.Session(graph=graph) as session:\n","  tf.global_variables_initializer().run()\n","  print(\"Initialized\")\n","  for step in range(num_steps):\n","    # Pick an offset within the training data, which has been randomized.\n","    # Note: we could use better randomization across epochs.\n","    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n","    # Generate a minibatch.\n","    batch_data = train_dataset[offset:(offset + batch_size), :]\n","    batch_labels = train_labels[offset:(offset + batch_size), :]\n","    # Prepare a dictionary telling the session where to feed the minibatch.\n","    # The key of the dictionary is the placeholder node of the graph to be fed,\n","    # and the value is the numpy array to feed to it.\n","    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n","    _, l, predictions = session.run(\n","      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n","    if (step % 500 == 0):\n","      print(\"Minibatch loss at step %d: %f\" % (step, l))\n","      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n","      print(\"Validation accuracy: %.1f%%\" % accuracy(\n","        valid_prediction.eval(), valid_labels))\n","  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Initialized\n","Minibatch loss at step 0: 401.035675\n","Minibatch accuracy: 11.7%\n","Validation accuracy: 36.5%\n","Minibatch loss at step 500: 13.935942\n","Minibatch accuracy: 82.0%\n","Validation accuracy: 77.9%\n","Minibatch loss at step 1000: 10.807089\n","Minibatch accuracy: 82.0%\n","Validation accuracy: 80.5%\n","Minibatch loss at step 1500: 12.437805\n","Minibatch accuracy: 79.7%\n","Validation accuracy: 81.1%\n","Minibatch loss at step 2000: 9.949905\n","Minibatch accuracy: 75.8%\n","Validation accuracy: 81.6%\n","Minibatch loss at step 2500: 11.923222\n","Minibatch accuracy: 75.0%\n","Validation accuracy: 79.5%\n","Minibatch loss at step 3000: 4.684561\n","Minibatch accuracy: 78.1%\n","Validation accuracy: 82.1%\n","Test accuracy: 88.3%\n"],"name":"stdout"}]}]}